sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_2x512,4x256,2x512 training.optimizer.lr=0.0001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_2x512,4x256,2x512 training.optimizer.lr=0.0002
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_2x512,4x256,2x512 training.optimizer.lr=0.0005
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_2x512,4x256,2x512 training.optimizer.lr=0.001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_2x512,4x256,2x512 training.optimizer.lr=0.002

sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_8x512 training.optimizer.lr=0.0001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_8x512 training.optimizer.lr=0.0002
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_8x512 training.optimizer.lr=0.0005
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_8x512 training.optimizer.lr=0.001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_cosformer_8x512 training.optimizer.lr=0.002

sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_8x512 training.optimizer.lr=0.0001 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_8x512 training.optimizer.lr=0.0002 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_8x512 training.optimizer.lr=0.0005 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_8x512 training.optimizer.lr=0.001 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_8x512 training.optimizer.lr=0.002 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true

sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_2x512,4x256,2x512 training.optimizer.lr=0.0001 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_2x512,4x256,2x512 training.optimizer.lr=0.0002 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_2x512,4x256,2x512 training.optimizer.lr=0.0005 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_2x512,4x256,2x512 training.optimizer.lr=0.001 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_vanilla_2x512,4x256,2x512 training.optimizer.lr=0.002 model.hourglass.attention_downsampling=true model.hourglass.attention_upsampling=true

sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_8 training.optimizer.lr=0.0001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_8 training.optimizer.lr=0.0002
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_8 training.optimizer.lr=0.0005
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_8 training.optimizer.lr=0.001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_8 training.optimizer.lr=0.002

sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_2x4x2 training.optimizer.lr=0.0001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_2x4x2 training.optimizer.lr=0.0002
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_2x4x2 training.optimizer.lr=0.0005
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_2x4x2 training.optimizer.lr=0.001
sbatch --gres=gpu:1 --cpus-per-task=1 --ntasks=1 --mem=120G --time=2-0 --partition=plgrid-gpu-a100 --account=plgllmparamgr-gpu-a100 poetry run python3 train_single_gpu.py --config-name=copying_mamba_2x4x2 training.optimizer.lr=0.002


